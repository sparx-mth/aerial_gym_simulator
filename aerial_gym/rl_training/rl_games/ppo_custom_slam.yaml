# RL Games config YAML for custom_slam_task
# Matches runner.py -> config["params"]["config"][...]

params:
  seed: 42

  # Environment + run config (the one runner.py edits)
  config:
    env_name: custom_slam_task              # must match your task_registry key
    env_config:
      num_envs: 3                         # overridden by --num_envs
      headless: False                   # overridden by --headless
      use_warp: True                        # overridden by --use_warp
    name: custom_slam_experiment

    # PPO/A2C hyperparameters
    reward_shaper:
      scale_value: 0.1
    normalize_advantage: True
    gamma: 0.98
    tau: 0.95
    ppo: True
    learning_rate: 1e-4
    lr_schedule: adaptive
    kl_threshold: 0.016
    save_best_after: 10
    score_to_win: 100000
    grad_norm: 1.0
    entropy_coef: 0.01
    truncate_grads: True
    e_clip: 0.2
    clip_value: False
    num_actors: 3                         # keep in sync with env_config.num_envs
    horizon_length: 64
    minibatch_size: 192
    mini_epochs: 4
    critic_coef: 2
    normalize_input: True
    bounds_loss_coef: 0.0001
    max_epochs: 500
    normalize_value: True
    use_diagnostics: True
    value_bootstrap: True

    player:
      render: False
      deterministic: True
      games_num: 10
      save_video: False
      video_path: "videos"

  # Algorithm selector (rl_games)
  algo:
    name: a2c_continuous

  # Model head
  model:
    name: continuous_a2c_logstd

  load_checkpoint: False

  # Viewer
  viewer:
    name: default
    headless: True
    use_warp: True
    render_mode: "human"  # or "rgb_array" for video capture

  # Policy/critic network
  network:
    name: actor_critic
    separate: False
    space:
      continuous:
        mu_activation: None
        sigma_activation: None
        mu_init: { name: default }
        sigma_init: { name: const_initializer, val: -0.5 }
        fixed_sigma: False
        initial_log_sigma: -0.5
        bounds_log_sigma: [-2.0, 1.0]   # keep std in [~0.14, ~2.7]

    mlp:
      units: [256, 128, 64]
      d2rl: False
      activation: elu
      initializer: { name: default, scale: 2 }
    # Optional memory for SLAM/POMDP:
    # rnn:
    #   name: gru
    #   units: 64
    #   layers: 1
    #   before_mlp: False
    #   layer_norm: True
